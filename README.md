# Neural-Network

## Lab01

#### Og√≥lny przeglƒÖd danych

Additional Information

This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In
particular, the Cleveland database is the only one that has been used by ML researchers to date. The "goal" field refers
to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the
Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value
0).

warto≈õci liczbowe atrybutu num:
-0 brak oznak choroby
-1,2,3,4 wystƒôpujƒÖce oznaki choroby

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>role</th>
      <th>type</th>
      <th>demographic</th>
      <th>description</th>
      <th>units</th>
      <th>missing_values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>age</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>Age</td>
      <td>None</td>
      <td>years</td>
      <td>no</td>
    </tr>
    <tr>
      <th>1</th>
      <td>sex</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>Sex</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>2</th>
      <td>cp</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trestbps</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>resting blood pressure (on admission to the ho...</td>
      <td>mm Hg</td>
      <td>no</td>
    </tr>
    <tr>
      <th>4</th>
      <td>chol</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>serum cholestoral</td>
      <td>mg/dl</td>
      <td>no</td>
    </tr>
    <tr>
      <th>5</th>
      <td>fbs</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>fasting blood sugar &gt; 120 mg/dl</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>6</th>
      <td>restecg</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>7</th>
      <td>thalach</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>maximum heart rate achieved</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>8</th>
      <td>exang</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>exercise induced angina</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>9</th>
      <td>oldpeak</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>ST depression induced by exercise relative to ...</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>10</th>
      <td>slope</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>11</th>
      <td>ca</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>number of major vessels (0-3) colored by flour...</td>
      <td>None</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>12</th>
      <td>thal</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>13</th>
      <td>num</td>
      <td>Target</td>
      <td>Integer</td>
      <td>None</td>
      <td>diagnosis of heart disease</td>
      <td>None</td>
      <td>no</td>
    </tr>
  </tbody>
</table>
</div>

<div>
    X
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>cp</th>
      <th>trestbps</th>
      <th>chol</th>
      <th>fbs</th>
      <th>restecg</th>
      <th>thalach</th>
      <th>exang</th>
      <th>oldpeak</th>
      <th>slope</th>
      <th>ca</th>
      <th>thal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>63</td>
      <td>1</td>
      <td>1</td>
      <td>145</td>
      <td>233</td>
      <td>1</td>
      <td>2</td>
      <td>150</td>
      <td>0</td>
      <td>2.3</td>
      <td>3</td>
      <td>0.0</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>67</td>
      <td>1</td>
      <td>4</td>
      <td>160</td>
      <td>286</td>
      <td>0</td>
      <td>2</td>
      <td>108</td>
      <td>1</td>
      <td>1.5</td>
      <td>2</td>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>67</td>
      <td>1</td>
      <td>4</td>
      <td>120</td>
      <td>229</td>
      <td>0</td>
      <td>2</td>
      <td>129</td>
      <td>1</td>
      <td>2.6</td>
      <td>2</td>
      <td>2.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>37</td>
      <td>1</td>
      <td>3</td>
      <td>130</td>
      <td>250</td>
      <td>0</td>
      <td>0</td>
      <td>187</td>
      <td>0</td>
      <td>3.5</td>
      <td>3</td>
      <td>0.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>41</td>
      <td>0</td>
      <td>2</td>
      <td>130</td>
      <td>204</td>
      <td>0</td>
      <td>2</td>
      <td>172</td>
      <td>0</td>
      <td>1.4</td>
      <td>1</td>
      <td>0.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>298</th>
      <td>45</td>
      <td>1</td>
      <td>1</td>
      <td>110</td>
      <td>264</td>
      <td>0</td>
      <td>0</td>
      <td>132</td>
      <td>0</td>
      <td>1.2</td>
      <td>2</td>
      <td>0.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>299</th>
      <td>68</td>
      <td>1</td>
      <td>4</td>
      <td>144</td>
      <td>193</td>
      <td>1</td>
      <td>0</td>
      <td>141</td>
      <td>0</td>
      <td>3.4</td>
      <td>2</td>
      <td>2.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>300</th>
      <td>57</td>
      <td>1</td>
      <td>4</td>
      <td>130</td>
      <td>131</td>
      <td>0</td>
      <td>0</td>
      <td>115</td>
      <td>1</td>
      <td>1.2</td>
      <td>2</td>
      <td>1.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>301</th>
      <td>57</td>
      <td>0</td>
      <td>2</td>
      <td>130</td>
      <td>236</td>
      <td>0</td>
      <td>2</td>
      <td>174</td>
      <td>0</td>
      <td>0.0</td>
      <td>2</td>
      <td>1.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>302</th>
      <td>38</td>
      <td>1</td>
      <td>3</td>
      <td>138</td>
      <td>175</td>
      <td>0</td>
      <td>0</td>
      <td>173</td>
      <td>0</td>
      <td>0.0</td>
      <td>1</td>
      <td>NaN</td>
      <td>3.0</td>
    </tr>
  </tbody>
</table>
</div>


<div>
y
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>298</th>
      <td>1</td>
    </tr>
    <tr>
      <th>299</th>
      <td>2</td>
    </tr>
    <tr>
      <th>300</th>
      <td>3</td>
    </tr>
    <tr>
      <th>301</th>
      <td>1</td>
    </tr>
    <tr>
      <th>302</th>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>303 rows √ó 1 columns</p>
</div>

#### 1.Czy zbi√≥r jest zbalansowany pod wzglƒôdem liczby pr√≥bek na klasy?

<table>
  <tr>
    <td><img src="./media/Lab01_files/Lab02_11_1.png" alt="Obraz 1"></td>
    <td><img src="./media/Lab01_files/Lab02_12_1.png" alt="Obraz 2"></td>
  </tr>
</table>

Mo≈ºna zauwa≈ºyƒá, ≈ºe najwiƒôcej przypadk√≥w jest dla warto≈õci 0, kt√≥ra stanowi ponad po≈Çowƒô wszystkich warto≈õci, je≈õli
chodzi o pozosta≈Çe, przypadek 1 posiada r√≥wnie≈º du≈ºy wk≈Çad, 2 i 3 majƒÖ prawie takƒÖ samƒÖ czƒôstotliwo≈õƒá na poziomie
ok.12%, przypadek 4 jest najmniej liczny i stanowi nieca≈Çe 5%.

Odpowied≈∫:
Zbi√≥r danych nie jest najlepiej zbalansowany, poniewa≈º niekt√≥re klasy majƒÖ znacznie wiƒôcej pr√≥bek ni≈º inne.

<table>
  <tr>
    <td><img src="./media/Lab01_files/Lab02_16_1.png" alt="Obraz 1"></td>
    <td><img src="./media/Lab01_files/Lab02_17_1.png" alt="Obraz 2"></td>
  </tr>
</table>

Je≈õli jednak p√≥j≈õƒá dalej i zobaczyƒá na warto≈õci atrybutu num w perspektywie - 'ma objawy' 'nie ma objawow', rozk≈Çad
bƒôdzie bardziej zbalansowany.

#### 2. Jakie sƒÖ ≈õrednie i odchylenia cech liczbowych?

    0          age
    3     trestbps
    4         chol
    7      thalach
    9      oldpeak
    11          ca
    Name: name, dtype: object

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>trestbps</th>
      <th>chol</th>
      <th>thalach</th>
      <th>oldpeak</th>
      <th>ca</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>54.438944</td>
      <td>131.689769</td>
      <td>246.693069</td>
      <td>149.607261</td>
      <td>1.039604</td>
      <td>0.672241</td>
    </tr>
    <tr>
      <th>std</th>
      <td>9.038662</td>
      <td>17.599748</td>
      <td>51.776918</td>
      <td>22.875003</td>
      <td>1.161075</td>
      <td>0.937438</td>
    </tr>
  </tbody>
</table>
</div>

#### 3. Dla cech liczbowych: czy ich rozk≈Çad jest w przybli≈ºeniu normalny?


![png](./media/Lab01_files/Lab02_25_0.png)

![png](./media/Lab01_files/Lab02_25_1.png)

![png](./media/Lab01_files/Lab02_25_2.png)

![png](./media/Lab01_files/Lab02_25_3.png)

![png](./media/Lab01_files/Lab02_25_4.png)

![png](./media/Lab01_files/Lab02_25_5.png)

![png](./media/Lab01_files/Lab02_27_0.png)

![png](./media/Lab01_files/Lab02_27_1.png)

![png](./media/Lab01_files/Lab02_27_2.png)

![png](./media/Lab01_files/Lab02_27_3.png)

![png](./media/Lab01_files/Lab02_27_4.png)

![png](./media/Lab01_files/Lab02_27_5.png)

BadajƒÖc te kwestiƒô postanowi≈Çem sprawdziƒá najpierw testem Shapiro-Wilka, czy warto≈õci sƒÖ rozdystrybuowane w spos√≥b
normalny, jednak patrzƒÖc na histogramy danych i wyniki testu postanowi≈Çem sprawdziƒá czy dane sƒÖ w 'przybli≈ºeniu'
rozdystrybuowane w spos√≥b normalny, wiƒôc postanowi≈Çem sprawdziƒá mniej restrykcyjnym testem. Wykres kwantylowy (qqplot).
Mo≈ºna z tego wyciƒÖgnƒÖƒá, ≈ºe atrybuty:

- thalach
- chol
- age

majƒÖ rozk≈Çad podobny do normalnego.

#### 4. Dla cech kategorycznych: czy rozk≈Çad jest w przybli≈ºeniu r√≥wnomierny?


    1         sex
    2          cp
    5         fbs
    6     restecg
    8       exang
    10      slope
    12       thal
    Name: name, dtype: object


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>cp</th>
      <th>fbs</th>
      <th>restecg</th>
      <th>exang</th>
      <th>slope</th>
      <th>thal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>298</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>299</th>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>300</th>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>301</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>302</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3.0</td>
    </tr>
  </tbody>
</table>
<p>303 rows √ó 7 columns</p>
</div>

| Obraz 1 | Obraz 2 |
|---------|---------|
| ![Obraz 1](./media/Lab01_files/Lab02_32_0.png) | ![Obraz 2](./media/Lab01_files/Lab02_32_1.png) |
| Obraz 3 | Obraz 4 |
| ![Obraz 3](./media/Lab01_files/Lab02_32_2.png) | ![Obraz 4](./media/Lab01_files/Lab02_32_3.png) |
| Obraz 5 | Obraz 6 |
| ![Obraz 5](./media/Lab01_files/Lab02_32_4.png) | ![Obraz 6](./media/Lab01_files/Lab02_32_5.png) |
| Obraz 7 |
| ![Obraz 7](./media/Lab01_files/Lab02_32_6.png) |


Rozk≈Çad warto≈õci atrybut√≥w kategorycznych jest r√≥≈ºny, w wiƒôkszo≈õci przypadk√≥w nier√≥wnomierny, zale≈ºnie od atrybutu.
w niekt√≥rych przypadkach jest spora dysproporcja w danych, ale przewa≈ºnie dla jednego z 3 przypadk√≥w.
np. **thal,slope,restec**
W przypadku **sex, fbs** znacznie przewa≈ºa jedna kategoria.
Najbardziej r√≥wnomierny jest zbi√≥r **cp**

#### 5. Czy wystƒôpujƒÖ cechy brakujƒÖce i jakƒÖ strategiƒô mo≈ºemy zastosowaƒá ≈ºeby je zastƒÖpiƒá?
Dla X:

    ca          4
    thal        2
    age         0
    sex         0
    cp          0
    trestbps    0
    chol        0
    fbs         0
    restecg     0
    thalach     0
    exang       0
    oldpeak     0
    slope       0
    dtype: int64
Dla y:

    num    0
    dtype: int64

W przypadku **ca** uzupe≈Çniƒô brakujƒÖce warto≈õci modƒÖ. Poniewa≈º jest to warto≈õƒá numeryczna lecz przyjmuje warto≈õci dyskretne. W przypadku **thal**
uzupe≈Çniƒô je modƒÖ, poniewa≈º jest to atrybut kategoryczny.

## Lab02

Sieci Neuronowe

Wynikiem implementacji listy jest program podzielony na 3 pliki:

- Functions.py -- funkcje pomocnicze, dyskretyzacja danych,
  normalizacja danych, obliczenie metryk dla zestawu testowego.

- Main.py -- ≈õrodowisko testowe, w nim testuje wizualizuje
  zaimplementowane metody i funkcje.

- NeuralNetwork.py -- klasa prostej sieci neuronowej, opartej na
  gradiencie z entropii krzy≈ºowej.

Implementacja rozwiƒÖzania:

Wyj≈õcie w sieci by≈Ço implementowane na wz√≥r:

ùëù(ùë•) = ùúé(ùëäùë• + ùëè)

``` Python
    def p(self, x):
        argument = np.dot(x, self.W) + self.b
        return self.sigmoid(argument)
```

gdzie funkcja sigmoid to:

<div style="text-align:center;">

$\sigma(n) = \frac{1}{1 + e^{-n}}$
</div>

Co w pythonie mo≈ºe byƒá osiƒÖgniƒôte za pomocƒÖ funkcji
[expit(x)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html)
:

``` Python
    def sigmoid(self, n):
    return expit(n)
```

Jako funkcjƒô kosztu wykorzystujemy entropiƒô krzy≈ºowƒÖ:
<div style="text-align:center;">
    ùêø = ‚àíùë¶ ln ùëù(ùë•) ‚àí (1 ‚àí ùë¶) ln(1 ‚àí ùëù(ùë•))
</div>

``` Python
    def cross_entropy_loss(self, y, y_pred):
        epsilon = 1e-15
        loss = y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred +epsilon)
        return -np.sum(loss)
```

epsilon zosta≈Ç dodany poniewa≈º by≈Ç problem z liczeniem warto≈õci 0.

Implementacja gradientu bƒôdzie za pomocƒÖ pochodnej po wagach modelu, co
z entropii krzy≈ºowej daje:
<div style="display: flex; justify-content: center; align-items: center; height: 200px;">

$\frac{\partial L}{\partial w_i} = -(y - p(x))x$

$\frac{\partial L}{\partial w_i} = (p(x) - y)x$

</div>

``` Python
    def compute_gradient(self, X_train, y_train):
        y_pred = self.p(X_train)
        dz = y_pred - y_train
        dw = np.dot(X_train.T, dz)
        db = np.sum(dz)
        return dw, db
```

Model uczy siƒô na podstawie zmiany wag, tak aby i≈õƒá w stronƒô wyznaczonƒÖ
przez gradient. Implementacja:

ùë§~i~‚Ä≤ = ùë§~ùëñ~ ‚àí ùõº $\frac{\partial L\ }{\partial wi}$

``` Python
    dw, db = self.compute_gradient(X_train, y_train)
```

Aktualizacja wag i bias zgodnie z gradientem i wsp√≥≈Çczynnikiem
uczenia:

``` Python
    self.W -= self.learning_rate * dw
    self.b -= self.learning_rate * db
```

M√≥j model posiada r√≥wnie≈º 3 funkcje kt√≥re mogƒÖ go wyuczyƒá:

- Fit_model_covergence -- kt√≥ry m√≥wi o wystarczajƒÖco ma≈Çej zmianie aby
  przerwaƒá proces uczenia.

- Fit -- podstawowa wersja uczƒÖca model, przechodzƒÖca przez ca≈Çy zbi√≥r
  X razy.

- Fit_batches -- wersja rozszerzona o dzielenie zbioru na paczki o
  nadanej wielko≈õci, przechodzi przez zbi√≥r X razy.

Aby zmaksymalizowaƒá uczenie siƒô modelu, dane przed ka≈ºdƒÖ iteracjƒÖ sƒÖ
losowo mieszane.

Aby zobaczyƒá wp≈Çyw procesowania danych bƒôdƒô rozpatrywa≈Ç wszystkie wyniki
kontek≈õcie 3 metod procesowania:

- Normalizacja

- Dyskretyzacja

- Surowe dane

Hiperparametry i parametry zosta≈Çy wybrane dla ka≈ºdego osobno tak aby
zmaksymalizowaƒá ich potencja≈Ç.

Wyniki uczenia dla parametrow i hiperparametr√≥w:

**Surowe dane**

``` Python
    learning_rate_basic_without_b = 0.1
    learning_rate_basic_with_b = 0.001
    num_of_iterations_basic = 400
    batch_size = 100
```

| Obraz 1                            | Obraz 2                            | Obraz 3                            | Obraz 4                            |
|------------------------------------|------------------------------------|------------------------------------|------------------------------------|
| ![Obraz 1](media/Lab02_files/basic_data_1.png) | ![Obraz 2](media/Lab02_files/basic_data_2.png) | ![Obraz 3](media/Lab02_files/basic_data_3.png) | ![Obraz 4](media/Lab02_files/basic_data_4.png) |
| Obraz 5                            | Obraz 6                            | Obraz 7                            | Obraz 8                            |
| ![Obraz 5](media/Lab02_files/basic_data_5.png) | ![Obraz 6](media/Lab02_files/basic_data_6.png) | ![Obraz 7](media/Lab02_files/basic_data_7.png) | ![Obraz 8](media/Lab02_files/basic_data_8.png) |

Mo≈ºemy stƒÖd zauwa≈ºyƒá, ≈ºe dane paczkowane, majƒÖ lepszy wynik ale sƒÖ mniej
stabilne je≈õli chodzi o metryki i proces uczenia.

Jendak oba wyniki sƒÖ bardzo dobre, plasujƒÖ siƒô na poziomie \>=0.6 je≈õli
chodzi o wszystkie metryki, co jest lepsze ni≈º losowe zgadywanie.
Najlepsza

**Dane poddane dyskretyzacji:**

``` Python
learning_rate_discrete_without_b = 0.0005
learning_rate_discrete_with_b = 0.0005
batch_size = 64
num_of_iterations_discretization = 60
```

| Obraz 1                                 | Obraz 2                                 | Obraz 3                                 | Obraz 4                                 |
|-----------------------------------------|-----------------------------------------|-----------------------------------------|-----------------------------------------|
| ![Obraz 1](media/Lab02_files/discretize_data_1.png) | ![Obraz 2](media/Lab02_files/discretize_data_2.png) | ![Obraz 3](media/Lab02_files/discretize_data_3.png) | ![Obraz 4](media/Lab02_files/discretize_data_4.png) |
| Obraz 5                                 | Obraz 6                                 | Obraz 7                                 | Obraz 8                                 |
| ![Obraz 5](media/Lab02_files/discretize_data_5.png) | ![Obraz 6](media/Lab02_files/discretize_data_6.png) | ![Obraz 7](media/Lab02_files/discretize_data_7.png) | ![Obraz 8](media/Lab02_files/discretize_data_8.png) |

**Dane poddane normalizacji:**

``` Python
learning_rate_normalization_without_b = 0.001
learning_rate_normalization_with_b = 0.001
num_of_iterations_normalization = 200
batch_size= 128
```

| Obraz 1                                | Obraz 2                                | Obraz 3                                | Obraz 4                                |
|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|
| ![Obraz 1](./media/Lab02_files/normalize_data_1.png) | ![Obraz 2](media/Lab02_files/normalize_data_2.png) | ![Obraz 3](media/Lab02_files/normalize_data_3.png) | ![Obraz 4](./media/Lab02_files/normalize_data_4.png) |
| Obraz 5                                | Obraz 6                                | Obraz 7                                | Obraz 8                                |
| ![Obraz 5](media/Lab02_files/normalize_data_5.png) | ![Obraz 6](media/Lab02_files/normalize_data_6.png) | ![Obraz 7](media/Lab02_files/normalize_data_7.png) | ![Obraz 8](./media/Lab02_files/normalize_data_8.png) |

W tym przypadku mo≈ºemy zauwa≈ºyƒá, ≈ºe jest zdecydowanie mniej iteracji bo
tylko 60 i model siƒô stabilizuje, w przypadku paczkowania pomimo braku
wyra≈∫nej r√≥≈ºnicy na wykresie kosztu mo≈ºemy zauwa≈ºyƒá r√≥≈ºnicƒô w miarach.
Wszystkie testowe miary wskazujƒÖ ok. 90% poprawno≈õci, co jest wspania≈Çym
wynikiem.

W przypadku paczkowania mo≈ºemy tak≈ºe zauwa≈ºyƒá mniejsze wahania metryk.

Wnioski:

Dobranie odpowiednich parametr√≥w i hiperparametr√≥w odgrywa kluczowƒÖ rolƒô
w powodzeniu modelu. Jest to ciƒô≈ºkie bez znajomo≈õci metod na znalezienie
optmalnych wsp√≥≈Çczynnik√≥w, trzeba sprawdzaƒá to metodƒÖ pr√≥b i b≈Çƒôd√≥w.

Preprocessing te≈º odgrywa wa≈ºnƒÖ rolƒô w tym jak sprawuje siƒô model.

Nawet taki prosty model mo≈ºe sobie dobrze radziƒá z binarnƒÖ klasyfikacjƒÖ.

## Lab03

M√≥j kod zawierajƒÖcy implementacje MLP (MultiLayer Percepton) sk≈Çada siƒô
z dw√≥ch kalas:

-   Main.py -- u≈ºywany do testowania i pokazywania wynik√≥w.

-   MLPClassifier.py -- zawiera implementacje MLP.

Klasy korzystajƒÖ te≈º z niekt√≥rych funkcji poprzednio zdefiniowanych w
Lab02.

FunkcjƒÖ aktywacji jest funkcja sigmoid.

<div style="text-align:center;">

$\sigma(n) = \frac{1}{1 + e^{-n}}$

</div>

FunkcjƒÖ straty jest entropia krzy≈ºowa.
<div style="text-align:center;">
    $H(y, p) = -\sum_{i} (y_i \cdot \ln(p_i))$
</div>
Algorytm polega na trzech kluczowych funkcjach:

1.  Forward_propagation
    1.  Zastosowaƒá wektor wej≈õciowy x~p~=\[x~p1~, x~p2~ \...,x~pN~\]

    2.  Obliczyƒá warto≈õci sieciowego wej≈õcia dla jednostek warstw
        ukrytej: z = Sum(w\*x)

    3.  Obliczyƒá wyj≈õcie z warstwy ukrytej: a = f(z)

    4.  Przej≈õƒá do warstwy wyj≈õciowej. Obliczyƒá warto≈õƒá sieciowego
        wyj≈õcia dla ka≈ºdej jednostki. z~w~ = sum(w~w~\*a~w-1~)

    5.  Obliczyƒá wyj≈õcia y_pred = f(z~w~) je≈õli to ostatnia warstwa to
        u≈ºyƒá softmax.

2.  Backward_propagation
    1.  Obliczyƒá b≈ÇƒÖd dla jednostek wyj≈õciowych.
    2.  Obliczyƒá b≈Çƒôdy dla jednostek ukrytych.
3.  Update_waights
    1. Uaktualniƒá wagi w warstwie wyj≈õciowej.
    2. Uaktualniƒá wagi w warstwie ukrytej.





Problem kt√≥ry staram siƒô sklasyfikowaƒá to czy osoba z podanymi
parametrami jest:

    - chora w stopniu - 2, 1 
    - jest zdrowa - 0. 
Wiƒôc moim problemem teraz jest klasyfikacja wieloklasowa.

Model bƒôdzie testowany ze wzglƒôdu na:

1.  Danych znormalizowanych i nieznormalizowanych

!Po tej czƒô≈õci bƒôdƒô bra≈Ç pod uwagƒô tylko dane znormalizowane.!

2.  R√≥≈ºnej wymiarowo≈õci warstwy ukrytej

3.  R√≥≈ºnej warto≈õci wsp√≥≈Çczynnika uczenia

4.  R√≥≈ºnych odchyle≈Ñ standardowych przy inicjalizacji wag

5.  R√≥≈ºnej liczby warstw

### 1. Zacznƒô od pokazania r√≥≈ºnic w danych znormalizowanych, surowych i zdyskretyzowanych .

Hiperparametry:

learning_rate = 0.001\
num_of_iterations = 400\
batch_size = 100\
hidden_layers = (4,)

| Obraz 1                                       | Obraz 2                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image007.png)        | ![png](media/Lab03_files/image008.png)       |

| Obraz 3                                       | Obraz 4                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image009.png)        | ![png](media/Lab03_files/image010.png)       |

| Obraz 5                                       | Obraz 6                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image011.png)        | ![png](media/Lab03_files/image012.png)       |

Wiƒôc mo≈ºemy po tym zauwa≈ºyƒá, ≈ºe dla danych znormalizowanych wyniki
wychodzƒÖ najlepiej, na nastepnym miejscu sƒÖ dane nieprzerobione. Po
metrykach mo≈ºemy zauwa≈ºyƒá, ze model ma zbyt wiele epok, poniewa≈º pomimo
spadku kosztu w trenowaniu, modele bardzo wolno siƒô podnoszƒÖ, o ile w
og√≥le, je≈õli chodzi o metryki.

2.  R√≥≈ºnej wymiarowo≈õci warstwy ukrytej.

learning_rate = 0.001\
num_of_iterations = 2000\
batch_size = 64\
hidden_layers = (X,)

| Obraz 1                                       | Obraz 2                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image007.png)        | ![png](media/Lab03_files/image008.png)       |

| Obraz 3                                       | Obraz 4                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image009.png)        | ![png](media/Lab03_files/image010.png)       |

| Obraz 5                                       | Obraz 6                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image011.png)        | ![png](media/Lab03_files/image012.png)       |


Z tych test√≥w mo≈ºemy zauwa≈ºyƒá, ≈ºe wiƒôksza liczba neuron√≥w w jednej i
jedynej warstwie ukrytej skutkuje lepszym podejmowaniem decyzji, krzywa
uczenia schodzi ni≈ºej z kosztem, oraz model podejmuje lepsze decyzje,
patrzƒÖc na miary.

Przy 4 neuronach jego miary plasowa≈Çy siƒô pomiƒôdzy \[0.55,0.7), a jego
ko≈Ñcowy koszt by≈Ç ok. 180.

Przy 64 neuronach jego miary by≈Çy ju≈º \>0.6, a co wa≈ºniejsze koszt by≈Ç w
okolicach 130.

Przy 64\> neuronach wyniki zaczƒô≈Çy spadaƒá.

3.  R√≥≈ºnej warto≈õci wsp√≥≈Çczynnika uczenia

learning_rate = X\
num_of_iterations = 2000\
batch_size = 64\
hidden_layers = (64,)

| Obraz 1                                       | Obraz 2                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image011.png)        | ![png](media/Lab03_files/image012.png)       |

| Obraz 3                                       | Obraz 4                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image013.png)        | ![png](media/Lab03_files/image014.png)       |

| Obraz 5                                       | Obraz 6                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image015.png)        | ![png](media/Lab03_files/image016.png)       |


Jak mo≈ºemy zauwa≈ºyƒá na zdjƒôciach, wsp√≥≈Çczynnik uczenia ma wielki wp≈Çyw
na proces uczenia.

Je≈õli go zwiƒôkszymy model bƒôdzie mniej stabilny, co mo≈ºna zauwa≈ºyƒá na
ostatnich zdjƒôciach.

Krzywa uczenia czƒôsto siƒô stabilizuje, a nawet wzrasta po kolejnych
iteracjach z du≈ºym wsp√≥≈Çczynnikiem.

W tym wypadku (z tymi hiperparametrami) najepiej prezentuje siƒô
wsp√≥≈Çczynnik \<=0.001.

4.  R√≥≈ºnych odchyle≈Ñ standardowych przy inicjalizacji wag. Tutaj
    przetestuje losowanie wag z r√≥≈ºnymi rozk≈Çadami. Wcze≈õniej wagi by≈Çy
    losowane pseudolosowo czyli za pomocƒÖ funkcji rand.

Kod i obja≈õnienie dziƒôki artyku≈Çowi:
[link](https://www.linkedin.com/pulse/review-initializing-neural-network-random-weights-rambaksh-prajapati/).

Teraz bƒôdƒô je rozlosowywa≈Ç na 3 r√≥≈ºne sposoby:

-   **Losowa inicjalizacja normalna:** W tej metodzie wagi sƒÖ inicjowane
    losowo z rozk≈Çadu normalnego o okre≈õlonej ≈õredniej i odchyleniu
    standardowym. Na przyk≈Çad, je≈õli chcemy zainicjowaƒá wagi ze ≈õredniƒÖ
    0 i odchyleniem standardowym 0,1.

| Obraz 1                                       | Obraz 2                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image017.png)        | ![png](media/Lab03_files/image018.png)       |


-   **Inicjalizacja Xavier:** Metoda ta zosta≈Ça nazwana na cze≈õƒá
    badacza, kt√≥ry jƒÖ zaproponowa≈Ç, Xaviera Glorota. W tej metodzie wagi
    sƒÖ inicjowane przy u≈ºyciu rozk≈Çadu Gaussa ze ≈õredniƒÖ 0 i odchyleniem
    standardowym sqrt(1/n), gdzie n to liczba neuron√≥w wej≈õciowych.
    Inicjalizacja Xaviera jest skuteczna, gdy funkcja aktywacji jest
    tangensem hiperbolicznym lub funkcjƒÖ sigmoidalnƒÖ.

| Obraz 1                                       | Obraz 2                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image019.png)        | ![png](media/Lab03_files/image020.png)       |


-   **Inicjalizacja He:** Nazwa tej metody pochodzi od nazwiska badacza,
    kt√≥ry jƒÖ zaproponowa≈Ç, Kaiming He. W tej metodzie wagi sƒÖ inicjowane
    przy u≈ºyciu rozk≈Çadu Gaussa ze ≈õredniƒÖ 0 i odchyleniem standardowym
    sqrt(2/n), gdzie n to liczba neuron√≥w wej≈õciowych. Inicjalizacja He
    jest skuteczna, gdy funkcja aktywacji jest funkcjƒÖ ReLU.

| Obraz 1                                       | Obraz 2                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](media/Lab03_files/image021.png)        | ![png](media/Lab03_files/image022.png)       |


Jak mo≈ºemy zauwa≈ºyƒá wszystkie modele podobnie siƒô reprezentujƒÖ, z tƒÖ
liczbƒÖ epok ka≈ºdy model jest w stanie siƒô naprostowaƒá i skorygowaƒá swoje
wagi. Jednak przy dobrze dobranej metodzie inicjalizacji wag, widzimy:

-   Ni≈ºszy startowy koszt, funkcji straty.

-   Wy≈ºsze poczƒÖtkowe miary.

SƒÖ to wa≈ºne rzeczy, je≈õli nasz model ma ograniczony czas uczenia.

5.  R√≥≈ºnej liczby warstw

| Obraz 1                                       | Obraz 2                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](./media/Lab03_files/image023.png)        | ![png](./media/Lab03_files/image024.png)       |

| Obraz 3                                       | Obraz 4                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](./media/Lab03_files/image025.png)        | ![png](./media/Lab03_files/image026.png)       |

| Obraz 5                                       | Obraz 6                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](./media/Lab03_files/image027.png)        | ![png](./media/Lab03_files/image028.png)       |

| Obraz 7                                       | Obraz 8                                      |
|-----------------------------------------------|----------------------------------------------|
| ![png](./media/Lab03_files/image029.png)        | ![png](./media/Lab03_files/image030.png)       |


Mo≈ºna po tym zauwa≈ºyƒá, ≈ºe wielko≈õƒá, jak i ilo≈õƒá warstw ma znaczenie.
Wiƒôksza ilo≈õƒá warstw pozwala zoptymalizowaƒá koszt jak i poprawiƒá jako≈õƒá
metryk. Jednak zbyt wielka ilo≈õƒá neuron√≥w i warstw mo≈ºe ≈∫le wp≈ÇynƒÖƒá na
model, kt√≥ry po prostu zapamiƒôta rozwiƒÖzania nie generalizujƒÖc.
