# Neural-Network

Lab02\
Sieci Neuronowe

Wynikiem implementacji listy jest program podzielony na 3 pliki:

- Functions.py -- funkcje pomocnicze, dyskretyzacja danych,
  normalizacja danych, obliczenie metryk dla zestawu testowego.

- Main.py -- Å›rodowisko testowe, w nim testuje wizualizuje
  zaimplementowane metody i funkcje.

- NeuralNetwork.py -- klasa prostej sieci neuronowej, opartej na
  gradiencie z entropii krzyÅ¼owej.

Implementacja rozwiÄ…zania:

WyjÅ›cie w sieci byÅ‚o implementowane na wzÃ³r:

ğ‘(ğ‘¥) = ğœ(ğ‘Šğ‘¥ + ğ‘)

    def p(self, x):
        argument = np.dot(x, self.W) + self.b\
        return self.sigmoid(argument)

gdzie funkcja sigmoid to:

<div style="text-align:center;">
  $$ \sigma(n) = \frac{1}{1 + e^{-n}} $$
</div>

Co w pythonie moÅ¼e byÄ‡ osiÄ…gniÄ™te za pomocÄ… funkcji
[expit(x)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html)
:

    def sigmoid(self, n):\
    return expit(n)

Jako funkcjÄ™ kosztu wykorzystujemy entropiÄ™ krzyÅ¼owÄ…:
<div style="text-align:center;">
    ğ¿ = âˆ’ğ‘¦ ln ğ‘(ğ‘¥) âˆ’ (1 âˆ’ ğ‘¦) ln(1 âˆ’ ğ‘(ğ‘¥))
</div>

    def cross_entropy_loss(self, y, y_pred):
        epsilon = 1e-15
        loss = y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred +epsilon)
        return -np.sum(loss)

epsilon zostaÅ‚ dodany poniewaÅ¼ byÅ‚ problem z liczeniem wartoÅ›ci 0.

Implementacja gradientu bÄ™dzie za pomocÄ… pochodnej po wagach modelu, co
z entropii krzyÅ¼owej daje:
<div style="text-align:center;">
  $$
  \frac{\partial L}{\partial w_i} = -(y - p(x))x
  $$
  $$
  \frac{\partial L}{\partial w_i} = (p(x) - y)x
  $$
</div>

    def compute_gradient(self, X_train, y_train):\
        y_pred = self.p(X_train)\
        dz = y_pred - y_train\
        dw = np.dot(X_train.T, dz)\
        db = np.sum(dz)\
        return dw, db

Model uczy siÄ™ na podstawie zmiany wag, tak aby iÅ›Ä‡ w stronÄ™ wyznaczonÄ…
przez gradient. Implementacja:

ğ‘¤~i~â€² = ğ‘¤~ğ‘–~ âˆ’ ğ›¼ $\frac{\partial L\ }{\partial wi}$

    dw, db = self.compute_gradient(X_train, y_train)

Aktualizacja wag i bias zgodnie z gradientem i wspÃ³Å‚czynnikiem
uczenia:

    self.W -= self.learning_rate * dw
    self.b -= self.learning_rate * db

MÃ³j model posiada rÃ³wnieÅ¼ 3 funkcje ktÃ³re mogÄ… go wyuczyÄ‡:

- Fit_model_covergence -- ktÃ³ry mÃ³wi o wystarczajÄ…co maÅ‚ej zmianie aby
  przerwaÄ‡ proces uczenia.

- Fit -- podstawowa wersja uczÄ…ca model, przechodzÄ…ca przez caÅ‚y zbiÃ³r
  X razy.

- Fit_batches -- wersja rozszerzona o dzielenie zbioru na paczki o
  nadanej wielkoÅ›ci, przechodzi przez zbiÃ³r X razy.

Aby zmaksymalizowaÄ‡ uczenie siÄ™ modelu, dane przed kaÅ¼dÄ… iteracjÄ… sÄ…
losowo mieszane.

Aby zobaczyÄ‡ wpÅ‚yw procesowania danych bÄ™dÄ™ rozpatrywaÅ‚ wszystkie wyniki
kontekÅ›cie 3 metod procesowania:

- Normalizacja

- Dyskretyzacja

- Surowe dane

Hiperparametry i parametry zostaÅ‚y wybrane dla kaÅ¼dego osobno tak aby
zmaksymalizowaÄ‡ ich potencjaÅ‚.

Wyniki uczenia dla parametrow i hiperparametrÃ³w:

**Surowe dane**

    learning_rate_basic_without_b = 0.1
    learning_rate_basic_with_b = 0.001
    num_of_iterations_basic = 400
    batch_size = 100

![A graph with a line Description automatically
generated](media/basic_data_1.png){width="1.5384612860892388in"
height="1.1545067804024496in"}![A graph with colored lines and white
text Description automatically
generated](media/basic_data_2.png){width="1.5538462379702538in"
height="1.1727012248468942in"}![A graph with a line Description
automatically generated](media/basic_data_3.png){width="1.5769225721784776in"
height="1.1923884514435696in"}![A graph of a training metrics
Description automatically
generated](media/basic_data_4.png){width="1.5307688101487313in"
height="1.1597047244094487in"}

![A graph with blue lines Description automatically
generated](media/basic_data_5.png){width="1.561538713910761in"
height="1.1800634295713035in"}![A graph of different colored lines
Description automatically
generated](media/basic_data_6.png){width="1.561537620297463in"
height="1.1771216097987751in"}![A graph with a line Description
automatically generated](media/basic_data_7.png){width="1.6020122484689414in"
height="1.2in"}![A graph of a training metrics Description automatically
generated](media/basic_data_8.png){width="1.6in"
height="1.1968974190726158in"}

MoÅ¼emy stÄ…d zauwaÅ¼yÄ‡, Å¼e dane paczkowane, majÄ… lepszy wynik ale sÄ… mniej
stabilne jeÅ›li chodzi o metryki i proces uczenia.

Jendak oba wyniki sÄ… bardzo dobre, plasujÄ… siÄ™ na poziomie \>=0.6 jeÅ›li
chodzi o wszystkie metryki, co jest lepsze niÅ¼ losowe zgadywanie.
Najlepsza

**Dane poddane dyskretyzacji:**

  learning_rate_discrete_without_b = 0.0005/
  learning_rate_discrete_with_b = 0.0005/
  batch_size = 64/
  num_of_iterations_discretization = 60

![A graph with a line Description automatically
generated](media/discretize_data_1.png){width="1.5384612860892388in"
height="1.1545067804024496in"}![A graph with colored lines and white
text Description automatically
generated](media/discretize_data_2.png){width="1.5538462379702538in"
height="1.1727012248468942in"}![A graph with a line Description
automatically generated](media/discretize_data_3.png){width="1.5769225721784776in"
height="1.1923884514435696in"}![A graph of a training metrics
Description automatically
generated](media/discretize_data_4.png){width="1.5307688101487313in"
height="1.1597047244094487in"}

![A graph with blue lines Description automatically
generated](media/discretize_data_5.png){width="1.561538713910761in"
height="1.1800634295713035in"}![A graph of different colored lines
Description automatically
generated](media/discretize_data_6.png){width="1.561537620297463in"
height="1.1771216097987751in"}![A graph with a line Description
automatically generated](media/discretize_data_7.png){width="1.6020122484689414in"
height="1.2in"}![A graph of a training metrics Description automatically
generated](media/discretize_data_8.png){width="1.6in"
height="1.1968974190726158in"}


**Dane poddane normalizacji:**
  learning_rate_normalization_without_b = 0.001\
  learning_rate_normalization_with_b = 0.001\
  num_of_iterations_normalization = 200\
  batch_size= 128

![A graph with a line Description automatically
generated](media/normalize_data_1.png){width="1.813377077865267in"
height="1.3384612860892389in"}![A graph of different colored lines
Description automatically
generated](media/normalize_data_2.png){width="1.769628171478565in"
height="1.3230774278215223in"}![A graph of a training cost Description
automatically generated](media/normalize_data_3.png){width="1.7570931758530184in"
height="1.3153849518810148in"}![A graph of a training metrics
Description automatically
generated](media/normalize_data_4.png){width="1.7414840332458443in"
height="1.3076924759405075in"}![A graph with a line Description
automatically generated](media/normalize_data_5.png){width="1.7569444444444444in"
height="1.3218219597550307in"}![A graph of a training metrics
Description automatically generated with medium
confidence](media/normalize_data_6.png){width="1.8051049868766404in"
height="1.3461537620297463in"}![A graph with a line Description
automatically generated](media/normalize_data_7.png){width="1.7665955818022747in"
height="1.3307699037620297in"}![A graph of a training metrics
Description automatically
generated](media/normalize_data_8.png){width="1.7742454068241469in"
height="1.3307688101487314in"}

W tym przypadku moÅ¼emy zauwaÅ¼yÄ‡, Å¼e jest zdecydowanie mniej iteracji bo
tylko 60 i model siÄ™ stabilizuje, w przypadku paczkowania pomimo braku
wyraÅºnej rÃ³Å¼nicy na wykresie kosztu moÅ¼emy zauwaÅ¼yÄ‡ rÃ³Å¼nicÄ™ w miarach.
Wszystkie testowe miary wskazujÄ… ok. 90% poprawnoÅ›ci, co jest wspaniaÅ‚ym
wynikiem.

W przypadku paczkowania moÅ¼emy takÅ¼e zauwaÅ¼yÄ‡ mniejsze wahania metryk.

Wnioski:

Dobranie odpowiednich parametrÃ³w i hiperparametrÃ³w odgrywa kluczowÄ… rolÄ™
w powodzeniu modelu. Jest to ciÄ™Å¼kie bez znajomoÅ›ci metod na znalezienie
optmalnych wspÃ³Å‚czynnikÃ³w, trzeba sprawdzaÄ‡ to metodÄ… prÃ³b i bÅ‚Ä™dÃ³w.

Preprocessing teÅ¼ odgrywa waÅ¼nÄ… rolÄ™ w tym jak sprawuje siÄ™ model.

Nawet taki prosty model moÅ¼e sobie dobrze radziÄ‡ z binarnÄ… klasyfikacjÄ….

