# Neural-Network

## Lab02
Sieci Neuronowe

Wynikiem implementacji listy jest program podzielony na 3 pliki:

- Functions.py -- funkcje pomocnicze, dyskretyzacja danych,
  normalizacja danych, obliczenie metryk dla zestawu testowego.

- Main.py -- Å›rodowisko testowe, w nim testuje wizualizuje
  zaimplementowane metody i funkcje.

- NeuralNetwork.py -- klasa prostej sieci neuronowej, opartej na
  gradiencie z entropii krzyÅ¼owej.

Implementacja rozwiÄ…zania:

WyjÅ›cie w sieci byÅ‚o implementowane na wzÃ³r:

ğ‘(ğ‘¥) = ğœ(ğ‘Šğ‘¥ + ğ‘)

``` Python
    def p(self, x):
        argument = np.dot(x, self.W) + self.b
        return self.sigmoid(argument)
```
gdzie funkcja sigmoid to:

<div style="text-align:center;">
 
  $\sigma(n) = \frac{1}{1 + e^{-n}}$
</div>

Co w pythonie moÅ¼e byÄ‡ osiÄ…gniÄ™te za pomocÄ… funkcji
[expit(x)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html)
:

``` Python
    def sigmoid(self, n):
    return expit(n)
```
Jako funkcjÄ™ kosztu wykorzystujemy entropiÄ™ krzyÅ¼owÄ…:
<div style="text-align:center;">
    ğ¿ = âˆ’ğ‘¦ ln ğ‘(ğ‘¥) âˆ’ (1 âˆ’ ğ‘¦) ln(1 âˆ’ ğ‘(ğ‘¥))
</div>

``` Python
    def cross_entropy_loss(self, y, y_pred):
        epsilon = 1e-15
        loss = y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred +epsilon)
        return -np.sum(loss)
```
epsilon zostaÅ‚ dodany poniewaÅ¼ byÅ‚ problem z liczeniem wartoÅ›ci 0.

Implementacja gradientu bÄ™dzie za pomocÄ… pochodnej po wagach modelu, co
z entropii krzyÅ¼owej daje:
<div style="display: flex; justify-content: center; align-items: center; height: 200px;">
  
  $\frac{\partial L}{\partial w_i} = -(y - p(x))x$
  
  $\frac{\partial L}{\partial w_i} = (p(x) - y)x$
  
</div>

``` Python
    def compute_gradient(self, X_train, y_train):
        y_pred = self.p(X_train)
        dz = y_pred - y_train
        dw = np.dot(X_train.T, dz)
        db = np.sum(dz)
        return dw, db
```
Model uczy siÄ™ na podstawie zmiany wag, tak aby iÅ›Ä‡ w stronÄ™ wyznaczonÄ…
przez gradient. Implementacja:

ğ‘¤~i~â€² = ğ‘¤~ğ‘–~ âˆ’ ğ›¼ $\frac{\partial L\ }{\partial wi}$

``` Python
    dw, db = self.compute_gradient(X_train, y_train)
```

Aktualizacja wag i bias zgodnie z gradientem i wspÃ³Å‚czynnikiem
uczenia:

``` Python
    self.W -= self.learning_rate * dw
    self.b -= self.learning_rate * db
```
MÃ³j model posiada rÃ³wnieÅ¼ 3 funkcje ktÃ³re mogÄ… go wyuczyÄ‡:

- Fit_model_covergence -- ktÃ³ry mÃ³wi o wystarczajÄ…co maÅ‚ej zmianie aby
  przerwaÄ‡ proces uczenia.

- Fit -- podstawowa wersja uczÄ…ca model, przechodzÄ…ca przez caÅ‚y zbiÃ³r
  X razy.

- Fit_batches -- wersja rozszerzona o dzielenie zbioru na paczki o
  nadanej wielkoÅ›ci, przechodzi przez zbiÃ³r X razy.

Aby zmaksymalizowaÄ‡ uczenie siÄ™ modelu, dane przed kaÅ¼dÄ… iteracjÄ… sÄ…
losowo mieszane.

Aby zobaczyÄ‡ wpÅ‚yw procesowania danych bÄ™dÄ™ rozpatrywaÅ‚ wszystkie wyniki
kontekÅ›cie 3 metod procesowania:

- Normalizacja

- Dyskretyzacja

- Surowe dane

Hiperparametry i parametry zostaÅ‚y wybrane dla kaÅ¼dego osobno tak aby
zmaksymalizowaÄ‡ ich potencjaÅ‚.

Wyniki uczenia dla parametrow i hiperparametrÃ³w:

**Surowe dane**

``` Python
    learning_rate_basic_without_b = 0.1
    learning_rate_basic_with_b = 0.001
    num_of_iterations_basic = 400
    batch_size = 100
```
| Obraz 1                            | Obraz 2                            | Obraz 3                            | Obraz 4                            |
|------------------------------------|------------------------------------|------------------------------------|------------------------------------|
| ![Obraz 1](media/basic_data_1.png) | ![Obraz 2](media/basic_data_2.png) | ![Obraz 3](media/basic_data_3.png) | ![Obraz 4](media/basic_data_4.png) |
| Obraz 5                            | Obraz 6                            | Obraz 7                            | Obraz 8                            |
| ![Obraz 5](media/basic_data_5.png) | ![Obraz 6](media/basic_data_6.png) | ![Obraz 7](media/basic_data_7.png) | ![Obraz 8](media/basic_data_8.png) |

MoÅ¼emy stÄ…d zauwaÅ¼yÄ‡, Å¼e dane paczkowane, majÄ… lepszy wynik ale sÄ… mniej
stabilne jeÅ›li chodzi o metryki i proces uczenia.

Jendak oba wyniki sÄ… bardzo dobre, plasujÄ… siÄ™ na poziomie \>=0.6 jeÅ›li
chodzi o wszystkie metryki, co jest lepsze niÅ¼ losowe zgadywanie.
Najlepsza

**Dane poddane dyskretyzacji:**

``` Python
learning_rate_discrete_without_b = 0.0005
learning_rate_discrete_with_b = 0.0005
batch_size = 64
num_of_iterations_discretization = 60
```
| Obraz 1                                 | Obraz 2                                 | Obraz 3                                 | Obraz 4                                 |
|-----------------------------------------|-----------------------------------------|-----------------------------------------|-----------------------------------------|
| ![Obraz 1](media/discretize_data_1.png) | ![Obraz 2](media/discretize_data_2.png) | ![Obraz 3](media/discretize_data_3.png) | ![Obraz 4](media/discretize_data_4.png) |
| Obraz 5                                 | Obraz 6                                 | Obraz 7                                 | Obraz 8                                 |
| ![Obraz 5](media/discretize_data_5.png) | ![Obraz 6](media/discretize_data_6.png) | ![Obraz 7](media/discretize_data_7.png) | ![Obraz 8](media/discretize_data_8.png) |

**Dane poddane normalizacji:**

``` Python
learning_rate_normalization_without_b = 0.001
learning_rate_normalization_with_b = 0.001
num_of_iterations_normalization = 200
batch_size= 128
```
| Obraz 1                                | Obraz 2                                | Obraz 3                                | Obraz 4                                |
|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|
| ![Obraz 1](media/normalize_data_1.png) | ![Obraz 2](media/normalize_data_2.png) | ![Obraz 3](media/normalize_data_3.png) | ![Obraz 4](media/normalize_data_4.png) |
| Obraz 5                                | Obraz 6                                | Obraz 7                                | Obraz 8                                |
| ![Obraz 5](media/normalize_data_5.png) | ![Obraz 6](media/normalize_data_6.png) | ![Obraz 7](media/normalize_data_7.png) | ![Obraz 8](media/normalize_data_8.png) |

W tym przypadku moÅ¼emy zauwaÅ¼yÄ‡, Å¼e jest zdecydowanie mniej iteracji bo
tylko 60 i model siÄ™ stabilizuje, w przypadku paczkowania pomimo braku
wyraÅºnej rÃ³Å¼nicy na wykresie kosztu moÅ¼emy zauwaÅ¼yÄ‡ rÃ³Å¼nicÄ™ w miarach.
Wszystkie testowe miary wskazujÄ… ok. 90% poprawnoÅ›ci, co jest wspaniaÅ‚ym
wynikiem.

W przypadku paczkowania moÅ¼emy takÅ¼e zauwaÅ¼yÄ‡ mniejsze wahania metryk.

Wnioski:

Dobranie odpowiednich parametrÃ³w i hiperparametrÃ³w odgrywa kluczowÄ… rolÄ™
w powodzeniu modelu. Jest to ciÄ™Å¼kie bez znajomoÅ›ci metod na znalezienie
optmalnych wspÃ³Å‚czynnikÃ³w, trzeba sprawdzaÄ‡ to metodÄ… prÃ³b i bÅ‚Ä™dÃ³w.

Preprocessing teÅ¼ odgrywa waÅ¼nÄ… rolÄ™ w tym jak sprawuje siÄ™ model.

Nawet taki prosty model moÅ¼e sobie dobrze radziÄ‡ z binarnÄ… klasyfikacjÄ….

## Lab03
