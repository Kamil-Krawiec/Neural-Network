# Neural-Network

## Lab01

#### Og√≥lny przeglƒÖd danych

Additional Information

This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In
particular, the Cleveland database is the only one that has been used by ML researchers to date. The "goal" field refers
to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the
Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value
0).

warto≈õci liczbowe atrybutu num:
-0 brak oznak choroby
-1,2,3,4 wystƒôpujƒÖce oznaki choroby

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>role</th>
      <th>type</th>
      <th>demographic</th>
      <th>description</th>
      <th>units</th>
      <th>missing_values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>age</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>Age</td>
      <td>None</td>
      <td>years</td>
      <td>no</td>
    </tr>
    <tr>
      <th>1</th>
      <td>sex</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>Sex</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>2</th>
      <td>cp</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>3</th>
      <td>trestbps</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>resting blood pressure (on admission to the ho...</td>
      <td>mm Hg</td>
      <td>no</td>
    </tr>
    <tr>
      <th>4</th>
      <td>chol</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>serum cholestoral</td>
      <td>mg/dl</td>
      <td>no</td>
    </tr>
    <tr>
      <th>5</th>
      <td>fbs</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>fasting blood sugar &gt; 120 mg/dl</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>6</th>
      <td>restecg</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>7</th>
      <td>thalach</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>maximum heart rate achieved</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>8</th>
      <td>exang</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>exercise induced angina</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>9</th>
      <td>oldpeak</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>ST depression induced by exercise relative to ...</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>10</th>
      <td>slope</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>no</td>
    </tr>
    <tr>
      <th>11</th>
      <td>ca</td>
      <td>Feature</td>
      <td>Integer</td>
      <td>None</td>
      <td>number of major vessels (0-3) colored by flour...</td>
      <td>None</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>12</th>
      <td>thal</td>
      <td>Feature</td>
      <td>Categorical</td>
      <td>None</td>
      <td>None</td>
      <td>None</td>
      <td>yes</td>
    </tr>
    <tr>
      <th>13</th>
      <td>num</td>
      <td>Target</td>
      <td>Integer</td>
      <td>None</td>
      <td>diagnosis of heart disease</td>
      <td>None</td>
      <td>no</td>
    </tr>
  </tbody>
</table>
</div>

<div>
    X
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>cp</th>
      <th>trestbps</th>
      <th>chol</th>
      <th>fbs</th>
      <th>restecg</th>
      <th>thalach</th>
      <th>exang</th>
      <th>oldpeak</th>
      <th>slope</th>
      <th>ca</th>
      <th>thal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>63</td>
      <td>1</td>
      <td>1</td>
      <td>145</td>
      <td>233</td>
      <td>1</td>
      <td>2</td>
      <td>150</td>
      <td>0</td>
      <td>2.3</td>
      <td>3</td>
      <td>0.0</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>67</td>
      <td>1</td>
      <td>4</td>
      <td>160</td>
      <td>286</td>
      <td>0</td>
      <td>2</td>
      <td>108</td>
      <td>1</td>
      <td>1.5</td>
      <td>2</td>
      <td>3.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>67</td>
      <td>1</td>
      <td>4</td>
      <td>120</td>
      <td>229</td>
      <td>0</td>
      <td>2</td>
      <td>129</td>
      <td>1</td>
      <td>2.6</td>
      <td>2</td>
      <td>2.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>37</td>
      <td>1</td>
      <td>3</td>
      <td>130</td>
      <td>250</td>
      <td>0</td>
      <td>0</td>
      <td>187</td>
      <td>0</td>
      <td>3.5</td>
      <td>3</td>
      <td>0.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>41</td>
      <td>0</td>
      <td>2</td>
      <td>130</td>
      <td>204</td>
      <td>0</td>
      <td>2</td>
      <td>172</td>
      <td>0</td>
      <td>1.4</td>
      <td>1</td>
      <td>0.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>298</th>
      <td>45</td>
      <td>1</td>
      <td>1</td>
      <td>110</td>
      <td>264</td>
      <td>0</td>
      <td>0</td>
      <td>132</td>
      <td>0</td>
      <td>1.2</td>
      <td>2</td>
      <td>0.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>299</th>
      <td>68</td>
      <td>1</td>
      <td>4</td>
      <td>144</td>
      <td>193</td>
      <td>1</td>
      <td>0</td>
      <td>141</td>
      <td>0</td>
      <td>3.4</td>
      <td>2</td>
      <td>2.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>300</th>
      <td>57</td>
      <td>1</td>
      <td>4</td>
      <td>130</td>
      <td>131</td>
      <td>0</td>
      <td>0</td>
      <td>115</td>
      <td>1</td>
      <td>1.2</td>
      <td>2</td>
      <td>1.0</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>301</th>
      <td>57</td>
      <td>0</td>
      <td>2</td>
      <td>130</td>
      <td>236</td>
      <td>0</td>
      <td>2</td>
      <td>174</td>
      <td>0</td>
      <td>0.0</td>
      <td>2</td>
      <td>1.0</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>302</th>
      <td>38</td>
      <td>1</td>
      <td>3</td>
      <td>138</td>
      <td>175</td>
      <td>0</td>
      <td>0</td>
      <td>173</td>
      <td>0</td>
      <td>0.0</td>
      <td>1</td>
      <td>NaN</td>
      <td>3.0</td>
    </tr>
  </tbody>
</table>
</div>


<div>
y
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>num</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
    </tr>
    <tr>
      <th>298</th>
      <td>1</td>
    </tr>
    <tr>
      <th>299</th>
      <td>2</td>
    </tr>
    <tr>
      <th>300</th>
      <td>3</td>
    </tr>
    <tr>
      <th>301</th>
      <td>1</td>
    </tr>
    <tr>
      <th>302</th>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>303 rows √ó 1 columns</p>
</div>

#### 1.Czy zbi√≥r jest zbalansowany pod wzglƒôdem liczby pr√≥bek na klasy?

<table>
  <tr>
    <td><img src="./media/Lab01_files/Lab02_11_1.png" alt="Obraz 1"></td>
    <td><img src="./media/Lab01_files/Lab02_12_1.png" alt="Obraz 2"></td>
  </tr>
</table>

Mo≈ºna zauwa≈ºyƒá, ≈ºe najwiƒôcej przypadk√≥w jest dla warto≈õci 0, kt√≥ra stanowi ponad po≈Çowƒô wszystkich warto≈õci, je≈õli
chodzi o pozosta≈Çe, przypadek 1 posiada r√≥wnie≈º du≈ºy wk≈Çad, 2 i 3 majƒÖ prawie takƒÖ samƒÖ czƒôstotliwo≈õƒá na poziomie
ok.12%, przypadek 4 jest najmniej liczny i stanowi nieca≈Çe 5%.

Odpowied≈∫:
Zbi√≥r danych nie jest najlepiej zbalansowany, poniewa≈º niekt√≥re klasy majƒÖ znacznie wiƒôcej pr√≥bek ni≈º inne.

<table>
  <tr>
    <td><img src="./media/Lab01_files/Lab02_16_1.png" alt="Obraz 1"></td>
    <td><img src="./media/Lab01_files/Lab02_17_1.png" alt="Obraz 2"></td>
  </tr>
</table>

Je≈õli jednak p√≥j≈õƒá dalej i zobaczyƒá na warto≈õci atrybutu num w perspektywie - 'ma objawy' 'nie ma objawow', rozk≈Çad
bƒôdzie bardziej zbalansowany.

#### 2. Jakie sƒÖ ≈õrednie i odchylenia cech liczbowych?

    0          age
    3     trestbps
    4         chol
    7      thalach
    9      oldpeak
    11          ca
    Name: name, dtype: object

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>trestbps</th>
      <th>chol</th>
      <th>thalach</th>
      <th>oldpeak</th>
      <th>ca</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>mean</th>
      <td>54.438944</td>
      <td>131.689769</td>
      <td>246.693069</td>
      <td>149.607261</td>
      <td>1.039604</td>
      <td>0.672241</td>
    </tr>
    <tr>
      <th>std</th>
      <td>9.038662</td>
      <td>17.599748</td>
      <td>51.776918</td>
      <td>22.875003</td>
      <td>1.161075</td>
      <td>0.937438</td>
    </tr>
  </tbody>
</table>
</div>

#### 3. Dla cech liczbowych: czy ich rozk≈Çad jest w przybli≈ºeniu normalny?


![png](./media/Lab01_files/Lab02_25_0.png)

![png](./media/Lab01_files/Lab02_25_1.png)

![png](./media/Lab01_files/Lab02_25_2.png)

![png](./media/Lab01_files/Lab02_25_3.png)

![png](./media/Lab01_files/Lab02_25_4.png)

![png](./media/Lab01_files/Lab02_25_5.png)

![png](./media/Lab01_files/Lab02_27_0.png)

![png](./media/Lab01_files/Lab02_27_1.png)

![png](./media/Lab01_files/Lab02_27_2.png)

![png](./media/Lab01_files/Lab02_27_3.png)

![png](./media/Lab01_files/Lab02_27_4.png)

![png](./media/Lab01_files/Lab02_27_5.png)

BadajƒÖc te kwestiƒô postanowi≈Çem sprawdziƒá najpierw testem Shapiro-Wilka, czy warto≈õci sƒÖ rozdystrybuowane w spos√≥b
normalny, jednak patrzƒÖc na histogramy danych i wyniki testu postanowi≈Çem sprawdziƒá czy dane sƒÖ w 'przybli≈ºeniu'
rozdystrybuowane w spos√≥b normalny, wiƒôc postanowi≈Çem sprawdziƒá mniej restrykcyjnym testem. Wykres kwantylowy (qqplot).
Mo≈ºna z tego wyciƒÖgnƒÖƒá, ≈ºe atrybuty:

- thalach
- chol
- age

majƒÖ rozk≈Çad podobny do normalnego.

#### 4. Dla cech kategorycznych: czy rozk≈Çad jest w przybli≈ºeniu r√≥wnomierny?


    1         sex
    2          cp
    5         fbs
    6     restecg
    8       exang
    10      slope
    12       thal
    Name: name, dtype: object


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>cp</th>
      <th>fbs</th>
      <th>restecg</th>
      <th>exang</th>
      <th>slope</th>
      <th>thal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>3</td>
      <td>6.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>298</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>299</th>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>300</th>
      <td>1</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>301</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>2</td>
      <td>3.0</td>
    </tr>
    <tr>
      <th>302</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3.0</td>
    </tr>
  </tbody>
</table>
<p>303 rows √ó 7 columns</p>
</div>

| Obraz 1 | Obraz 2 |
|---------|---------|
| ![Obraz 1](./media/Lab01_files/Lab02_32_0.png) | ![Obraz 2](./media/Lab01_files/Lab02_32_1.png) |
| Obraz 3 | Obraz 4 |
| ![Obraz 3](./media/Lab01_files/Lab02_32_2.png) | ![Obraz 4](./media/Lab01_files/Lab02_32_3.png) |
| Obraz 5 | Obraz 6 |
| ![Obraz 5](./media/Lab01_files/Lab02_32_4.png) | ![Obraz 6](./media/Lab01_files/Lab02_32_5.png) |
| Obraz 7 |
| ![Obraz 7](./media/Lab01_files/Lab02_32_6.png) |


Rozk≈Çad warto≈õci atrybut√≥w kategorycznych jest r√≥≈ºny, w wiƒôkszo≈õci przypadk√≥w nier√≥wnomierny, zale≈ºnie od atrybutu.
w niekt√≥rych przypadkach jest spora dysproporcja w danych, ale przewa≈ºnie dla jednego z 3 przypadk√≥w.
np. **thal,slope,restec**
W przypadku **sex, fbs** znacznie przewa≈ºa jedna kategoria.
Najbardziej r√≥wnomierny jest zbi√≥r **cp**

#### 5. Czy wystƒôpujƒÖ cechy brakujƒÖce i jakƒÖ strategiƒô mo≈ºemy zastosowaƒá ≈ºeby je zastƒÖpiƒá?
Dla X:

    ca          4
    thal        2
    age         0
    sex         0
    cp          0
    trestbps    0
    chol        0
    fbs         0
    restecg     0
    thalach     0
    exang       0
    oldpeak     0
    slope       0
    dtype: int64
Dla y:

    num    0
    dtype: int64

W przypadku **ca** uzupe≈Çniƒô brakujƒÖce warto≈õci modƒÖ. Poniewa≈º jest to warto≈õƒá numeryczna lecz przyjmuje warto≈õci dyskretne. W przypadku **thal**
uzupe≈Çniƒô je modƒÖ, poniewa≈º jest to atrybut kategoryczny.

## Lab02

Sieci Neuronowe

Wynikiem implementacji listy jest program podzielony na 3 pliki:

- Functions.py -- funkcje pomocnicze, dyskretyzacja danych,
  normalizacja danych, obliczenie metryk dla zestawu testowego.

- Main.py -- ≈õrodowisko testowe, w nim testuje wizualizuje
  zaimplementowane metody i funkcje.

- NeuralNetwork.py -- klasa prostej sieci neuronowej, opartej na
  gradiencie z entropii krzy≈ºowej.

Implementacja rozwiƒÖzania:

Wyj≈õcie w sieci by≈Ço implementowane na wz√≥r:

ùëù(ùë•) = ùúé(ùëäùë• + ùëè)

``` Python
    def p(self, x):
        argument = np.dot(x, self.W) + self.b
        return self.sigmoid(argument)
```

gdzie funkcja sigmoid to:

<div style="text-align:center;">

$\sigma(n) = \frac{1}{1 + e^{-n}}$
</div>

Co w pythonie mo≈ºe byƒá osiƒÖgniƒôte za pomocƒÖ funkcji
[expit(x)](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html)
:

``` Python
    def sigmoid(self, n):
    return expit(n)
```

Jako funkcjƒô kosztu wykorzystujemy entropiƒô krzy≈ºowƒÖ:
<div style="text-align:center;">
    ùêø = ‚àíùë¶ ln ùëù(ùë•) ‚àí (1 ‚àí ùë¶) ln(1 ‚àí ùëù(ùë•))
</div>

``` Python
    def cross_entropy_loss(self, y, y_pred):
        epsilon = 1e-15
        loss = y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred +epsilon)
        return -np.sum(loss)
```

epsilon zosta≈Ç dodany poniewa≈º by≈Ç problem z liczeniem warto≈õci 0.

Implementacja gradientu bƒôdzie za pomocƒÖ pochodnej po wagach modelu, co
z entropii krzy≈ºowej daje:
<div style="display: flex; justify-content: center; align-items: center; height: 200px;">

$\frac{\partial L}{\partial w_i} = -(y - p(x))x$

$\frac{\partial L}{\partial w_i} = (p(x) - y)x$

</div>

``` Python
    def compute_gradient(self, X_train, y_train):
        y_pred = self.p(X_train)
        dz = y_pred - y_train
        dw = np.dot(X_train.T, dz)
        db = np.sum(dz)
        return dw, db
```

Model uczy siƒô na podstawie zmiany wag, tak aby i≈õƒá w stronƒô wyznaczonƒÖ
przez gradient. Implementacja:

ùë§~i~‚Ä≤ = ùë§~ùëñ~ ‚àí ùõº $\frac{\partial L\ }{\partial wi}$

``` Python
    dw, db = self.compute_gradient(X_train, y_train)
```

Aktualizacja wag i bias zgodnie z gradientem i wsp√≥≈Çczynnikiem
uczenia:

``` Python
    self.W -= self.learning_rate * dw
    self.b -= self.learning_rate * db
```

M√≥j model posiada r√≥wnie≈º 3 funkcje kt√≥re mogƒÖ go wyuczyƒá:

- Fit_model_covergence -- kt√≥ry m√≥wi o wystarczajƒÖco ma≈Çej zmianie aby
  przerwaƒá proces uczenia.

- Fit -- podstawowa wersja uczƒÖca model, przechodzƒÖca przez ca≈Çy zbi√≥r
  X razy.

- Fit_batches -- wersja rozszerzona o dzielenie zbioru na paczki o
  nadanej wielko≈õci, przechodzi przez zbi√≥r X razy.

Aby zmaksymalizowaƒá uczenie siƒô modelu, dane przed ka≈ºdƒÖ iteracjƒÖ sƒÖ
losowo mieszane.

Aby zobaczyƒá wp≈Çyw procesowania danych bƒôdƒô rozpatrywa≈Ç wszystkie wyniki
kontek≈õcie 3 metod procesowania:

- Normalizacja

- Dyskretyzacja

- Surowe dane

Hiperparametry i parametry zosta≈Çy wybrane dla ka≈ºdego osobno tak aby
zmaksymalizowaƒá ich potencja≈Ç.

Wyniki uczenia dla parametrow i hiperparametr√≥w:

**Surowe dane**

``` Python
    learning_rate_basic_without_b = 0.1
    learning_rate_basic_with_b = 0.001
    num_of_iterations_basic = 400
    batch_size = 100
```

| Obraz 1                            | Obraz 2                            | Obraz 3                            | Obraz 4                            |
|------------------------------------|------------------------------------|------------------------------------|------------------------------------|
| ![Obraz 1](media/Lab02_files/basic_data_1.png) | ![Obraz 2](media/Lab02_files/basic_data_2.png) | ![Obraz 3](media/Lab02_files/basic_data_3.png) | ![Obraz 4](media/Lab02_files/basic_data_4.png) |
| Obraz 5                            | Obraz 6                            | Obraz 7                            | Obraz 8                            |
| ![Obraz 5](media/Lab02_files/basic_data_5.png) | ![Obraz 6](media/Lab02_files/basic_data_6.png) | ![Obraz 7](media/Lab02_files/basic_data_7.png) | ![Obraz 8](media/Lab02_files/basic_data_8.png) |

Mo≈ºemy stƒÖd zauwa≈ºyƒá, ≈ºe dane paczkowane, majƒÖ lepszy wynik ale sƒÖ mniej
stabilne je≈õli chodzi o metryki i proces uczenia.

Jendak oba wyniki sƒÖ bardzo dobre, plasujƒÖ siƒô na poziomie \>=0.6 je≈õli
chodzi o wszystkie metryki, co jest lepsze ni≈º losowe zgadywanie.
Najlepsza

**Dane poddane dyskretyzacji:**

``` Python
learning_rate_discrete_without_b = 0.0005
learning_rate_discrete_with_b = 0.0005
batch_size = 64
num_of_iterations_discretization = 60
```

| Obraz 1                                 | Obraz 2                                 | Obraz 3                                 | Obraz 4                                 |
|-----------------------------------------|-----------------------------------------|-----------------------------------------|-----------------------------------------|
| ![Obraz 1](media/Lab02_files/discretize_data_1.png) | ![Obraz 2](media/Lab02_files/discretize_data_2.png) | ![Obraz 3](media/Lab02_files/discretize_data_3.png) | ![Obraz 4](media/Lab02_files/discretize_data_4.png) |
| Obraz 5                                 | Obraz 6                                 | Obraz 7                                 | Obraz 8                                 |
| ![Obraz 5](media/Lab02_files/discretize_data_5.png) | ![Obraz 6](media/Lab02_files/discretize_data_6.png) | ![Obraz 7](media/Lab02_files/discretize_data_7.png) | ![Obraz 8](media/Lab02_files/discretize_data_8.png) |

**Dane poddane normalizacji:**

``` Python
learning_rate_normalization_without_b = 0.001
learning_rate_normalization_with_b = 0.001
num_of_iterations_normalization = 200
batch_size= 128
```

| Obraz 1                                | Obraz 2                                | Obraz 3                                | Obraz 4                                |
|----------------------------------------|----------------------------------------|----------------------------------------|----------------------------------------|
| ![Obraz 1](media/Lab02_files/normalize_data_1.png) | ![Obraz 2](media/Lab02_files/normalize_data_2.png) | ![Obraz 3](media/Lab02_files/normalize_data_3.png) | ![Obraz 4](media/Lab02_files/normalize_data_4.png) |
| Obraz 5                                | Obraz 6                                | Obraz 7                                | Obraz 8                                |
| ![Obraz 5](media/Lab02_files/normalize_data_5.png) | ![Obraz 6](media/Lab02_files/normalize_data_6.png) | ![Obraz 7](media/Lab02_files/normalize_data_7.png) | ![Obraz 8](media/Lab02_files/normalize_data_8.png) |

W tym przypadku mo≈ºemy zauwa≈ºyƒá, ≈ºe jest zdecydowanie mniej iteracji bo
tylko 60 i model siƒô stabilizuje, w przypadku paczkowania pomimo braku
wyra≈∫nej r√≥≈ºnicy na wykresie kosztu mo≈ºemy zauwa≈ºyƒá r√≥≈ºnicƒô w miarach.
Wszystkie testowe miary wskazujƒÖ ok. 90% poprawno≈õci, co jest wspania≈Çym
wynikiem.

W przypadku paczkowania mo≈ºemy tak≈ºe zauwa≈ºyƒá mniejsze wahania metryk.

Wnioski:

Dobranie odpowiednich parametr√≥w i hiperparametr√≥w odgrywa kluczowƒÖ rolƒô
w powodzeniu modelu. Jest to ciƒô≈ºkie bez znajomo≈õci metod na znalezienie
optmalnych wsp√≥≈Çczynnik√≥w, trzeba sprawdzaƒá to metodƒÖ pr√≥b i b≈Çƒôd√≥w.

Preprocessing te≈º odgrywa wa≈ºnƒÖ rolƒô w tym jak sprawuje siƒô model.

Nawet taki prosty model mo≈ºe sobie dobrze radziƒá z binarnƒÖ klasyfikacjƒÖ.

#### Lab03
